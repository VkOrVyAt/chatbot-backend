import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import re
import os

def main():
    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    base_model_name = "ai-forever/rugpt3medium_based_on_gpt2"
    
    # –ò—â–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –∞–¥–∞–ø—Ç–µ—Ä—É
    possible_paths = [
        "models/faq_bot",
        "models/faq_bot/final_model", 
        "checkpoints/checkpoint_best",
        "checkpoints/checkpoint_last"
    ]
    
    peft_model_path = None
    for path in possible_paths:
        if os.path.exists(os.path.join(path, "adapter_config.json")):
            peft_model_path = path
            print(f"–ù–∞–π–¥–µ–Ω –∞–¥–∞–ø—Ç–µ—Ä –≤: {path}")
            break
    
    if peft_model_path is None:
        print("–ê–¥–∞–ø—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω! –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å.")
    
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
    # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞, –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    if peft_model_path and os.path.exists(os.path.join(peft_model_path, "tokenizer.json")):
        tokenizer = AutoTokenizer.from_pretrained(peft_model_path)
        print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞")
    else:
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏")
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º PEFT –∞–¥–∞–ø—Ç–µ—Ä...")
    if peft_model_path:
        try:
            model = PeftModel.from_pretrained(base_model, peft_model_path)
            print("–ê–¥–∞–ø—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–¥–∞–ø—Ç–µ—Ä–∞: {e}")
            print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞...")
            model = base_model
    else:
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞...")
        model = base_model
    
    model.eval()

    system_prompt = "–¢—ã ‚Äî UrFU FAQ‚Äë–±–æ—Ç. –û—Ç–≤–µ—á–∞–µ—à—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø—Ä–æ –£—Ä–§–£ –∏ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫—É—é –∂–∏–∑–Ω—å."
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω—ã
    stop_tokens = ["<<", "<|", "User", "System", "Assistant"]
    
    print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ú–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã!")
    print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit' –∏–ª–∏ 'quit'")
    
    while True:
        question = input("\n–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å: ").strip()
        if question.lower() in ("exit", "quit", "–≤—ã—Ö–æ–¥"):
            break
        
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–≤—è–∑–∞–Ω –ª–∏ –≤–æ–ø—Ä–æ—Å —Å –£—Ä–§–£
            urfu_keywords = ['—É—Ä—Ñ—É', '—É—Ä–∞–ª—å—Å–∫–∏–π', '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '—Å—Ç—É–¥–µ–Ω—Ç', '—É—á–µ–±–∞', 
                           '–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', '—ç–∫–∑–∞–º–µ–Ω', '–¥–µ–∫–∞–Ω–∞—Ç', '—Ñ–∞–∫—É–ª—å—Ç–µ—Ç', '–∫–∞—Ñ–µ–¥—Ä–∞', '–æ–±—â–µ–∂–∏—Ç–∏–µ',
                           '—Å—Ç–∏–ø–µ–Ω–¥–∏—è', '—Å–µ—Å—Å–∏—è', '–¥–∏–ø–ª–æ–º', '—Å–ø—Ä–∞–≤–∫–∞', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ']
            
            question_lower = question.lower()
            is_urfu_related = any(keyword in question_lower for keyword in urfu_keywords)
            
            # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ —Å–≤—è–∑–∞–Ω —Å –£—Ä–§–£, –¥–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –æ—Ç–≤–µ—Ç
            if not is_urfu_related:
                print("–û—Ç–≤–µ—Ç: –Ø –æ—Ç–≤–µ—á–∞—é —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –£—Ä–∞–ª—å—Å–∫–∏–º —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–º —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–º (–£—Ä–§–£). –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –æ –£—Ä–§–£, —É—á–µ–±–µ –∏–ª–∏ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–π –∂–∏–∑–Ω–∏.")
                continue

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –£—Ä–§–£
        prompt = f"""–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –£—Ä–∞–ª—å—Å–∫–æ–≥–æ —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–æ–≥–æ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ (–£—Ä–§–£).
–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –£—Ä–§–£, —É—á–µ–±–æ–π, –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ–º, —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–π –∂–∏–∑–Ω—å—é.
–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ —Å–≤—è–∑–∞–Ω —Å –£—Ä–§–£, –≤–µ–∂–ª–∏–≤–æ —Å–∫–∞–∂–∏, —á—Ç–æ –æ—Ç–≤–µ—á–∞–µ—à—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø—Ä–æ –£—Ä–§–£.

–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç:"""
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=512
        )
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                    do_sample=True,
                    temperature=0.3,    # –ï—â–µ –±–æ–ª—å—à–µ —Å–Ω–∏–∂–∞–µ–º –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    top_p=0.9,          # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                    top_k=50,           # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –≤—ã–±–æ—Ä–∞
                    repetition_penalty=1.2,  # –£–º–µ—Ä–µ–Ω–Ω—ã–π —à—Ç—Ä–∞—Ñ
                    no_repeat_ngram_size=2,
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.pad_token_id,
                    use_cache=True,
                    # –£–±–∏—Ä–∞–µ–º early_stopping - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
                )

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –ø—Ä–æ–º–ø—Ç–∞
            if "–û—Ç–≤–µ—Ç:" in full_text:
                answer = full_text.split("–û—Ç–≤–µ—Ç:", 1)[1].strip()
            else:
                answer = full_text[len(prompt):].strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –¥–æ –ø–µ—Ä–≤–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –∏–ª–∏ —Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            answer = answer.split("–í–æ–ø—Ä–æ—Å:")[0].strip()
            answer = answer.split("?")[0].strip()  # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –ø–µ—Ä–≤–æ–º –≤–æ–ø—Ä–æ—Å–µ
            if "?" in answer and len(answer) > 100:  # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å –∏ —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω—ã–π
                answer = answer.split("?")[0].strip()
            
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
            sentences = answer.split('.')
            if len(sentences) > 1 and len(answer) > 100:
                answer = sentences[0].strip() + '.'
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω–æ–≤
            for stop_token in stop_tokens:
                if stop_token in answer:
                    answer = answer.split(stop_token)[0].strip()
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            answer = re.sub(r'<[^>]+>', '', answer)  # –£–¥–∞–ª—è–µ–º XML-—Ç–µ–≥–∏
            answer = re.sub(r'\s+', ' ', answer)     # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
            answer = answer.strip()
            
            # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã –∏ –≤–æ–ø—Ä–æ—Å—ã
            answer = re.sub(r'–¢—ã –∑–Ω–∞–µ—à—å.*?', '', answer).strip()
            answer = re.sub(r'–ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ.*?', '', answer).strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ - –¥–µ–ª–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–æ–π
            bad_indicators = ['team', 'assistance', 'curriculum', 'partner']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∏ –∫–∞—á–µ—Å—Ç–≤–æ
            if (len(answer) < 3 or 
                answer.count('<<') > 0 or
                answer.count('?') > 3):  # –ú–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–¥—Ä—è–¥
                print("–û—Ç–≤–µ—Ç: –ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –æ –£—Ä–§–£. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.")
            else:
                print(f"–û—Ç–≤–µ—Ç: {answer}")
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            print("–û—Ç–≤–µ—Ç: –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞.")

def check_model_files():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏"""
    peft_path = "models/faq_bot"
    required_files = ["adapter_config.json", "adapter_model.safetensors"]
    
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏:")
    for file in required_files:
        file_path = os.path.join(peft_path, file)
        if os.path.exists(file_path):
            print(f"‚úì {file} –Ω–∞–π–¥–µ–Ω –≤ {peft_path}")
        else:
            print(f"‚úó {file} –ù–ï –Ω–∞–π–¥–µ–Ω –≤ {peft_path}")
    
    print(f"\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ {peft_path}:")
    if os.path.exists(peft_path):
        for item in os.listdir(peft_path):
            item_path = os.path.join(peft_path, item)
            if os.path.isdir(item_path):
                print(f"  üìÅ {item}/")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ–¥–ø–∞–ø–æ–∫
                try:
                    for subitem in os.listdir(item_path):
                        print(f"    - {subitem}")
                except:
                    pass
            else:
                print(f"  üìÑ {item}")
    else:
        print("  –ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")

if __name__ == "__main__":
    check_model_files()
    print("\n" + "="*50 + "\n")
    main()